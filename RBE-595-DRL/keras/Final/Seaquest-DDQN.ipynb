{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, numpy, math, gym, scipy\n",
    "from SumTree import SumTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 84\n",
    "IMAGE_HEIGHT = 84\n",
    "IMAGE_STACK = 2\n",
    "\n",
    "HUBER_LOSS_DELTA = 2.0\n",
    "LEARNING_RATE = 0.00025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(y_true, y_pred):\n",
    "    err = y_true - y_pred\n",
    "\n",
    "    cond = K.abs(err) < HUBER_LOSS_DELTA\n",
    "    L2 = 0.5 * K.square(err)\n",
    "    L1 = HUBER_LOSS_DELTA * (K.abs(err) - 0.5 * HUBER_LOSS_DELTA)\n",
    "\n",
    "    loss = tf.where(cond, L2, L1)   # Keras does not cover where function in tensorflow :-(\n",
    "\n",
    "    return K.mean(loss)\n",
    "\n",
    "def processImage( img ):\n",
    "    rgb = scipy.misc.imresize(img, (IMAGE_WIDTH, IMAGE_HEIGHT), interp='bilinear')\n",
    "\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b     # extract luminance\n",
    "\n",
    "    o = gray.astype('float32') / 128 - 1    # normalize\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a level 2 heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/schandrasekhar/envs/kerasenvp3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/schandrasekhar/envs/kerasenvp3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/schandrasekhar/envs/kerasenvp3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/schandrasekhar/envs/kerasenvp3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/schandrasekhar/envs/kerasenvp3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/schandrasekhar/envs/kerasenvp3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/schandrasekhar/envs/kerasenvp3.6/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/schandrasekhar/envs/kerasenvp3.6/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/schandrasekhar/envs/kerasenvp3.6/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/schandrasekhar/envs/kerasenvp3.6/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/schandrasekhar/envs/kerasenvp3.6/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/schandrasekhar/envs/kerasenvp3.6/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    def __init__(self, stateCnt, actionCnt):\n",
    "        self.stateCnt = stateCnt\n",
    "        self.actionCnt = actionCnt\n",
    "\n",
    "        self.model = self._createModel()\n",
    "        self.model_ = self._createModel()  # target network\n",
    "\n",
    "    def _createModel(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, (8, 8), strides=(4,4), activation='relu', input_shape=(self.stateCnt), data_format='channels_first'))\n",
    "        model.add(Conv2D(64, (4, 4), strides=(2,2), activation='relu'))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(units=512, activation='relu'))\n",
    "\n",
    "        model.add(Dense(units=actionCnt, activation='linear'))\n",
    "\n",
    "        opt = RMSprop(lr=LEARNING_RATE)\n",
    "        model.compile(loss=huber_loss, optimizer=opt)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self, x, y, epochs=1, verbose=0):\n",
    "        self.model.fit(x, y, batch_size=32, epochs=epochs, verbose=verbose)\n",
    "\n",
    "    def predict(self, s, target=False):\n",
    "        if target:\n",
    "            return self.model_.predict(s)\n",
    "        else:\n",
    "            return self.model.predict(s)\n",
    "\n",
    "    def predictOne(self, s, target=False):\n",
    "        return self.predict(s.reshape(1, IMAGE_STACK, IMAGE_WIDTH, IMAGE_HEIGHT), target).flatten()\n",
    "\n",
    "    def updateTargetModel(self):\n",
    "        self.model_.set_weights(self.model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:   # stored as ( s, a, r, s_ ) in SumTree\n",
    "    e = 0.01\n",
    "    a = 0.6\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    def _getPriority(self, error):\n",
    "        return (error + self.e) ** self.a\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.add(p, sample) \n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        segment = self.tree.total() / n\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            batch.append( (idx, data) )\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.update(idx, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_CAPACITY = 200000\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.1\n",
    "\n",
    "EXPLORATION_STOP = 5   # at this step epsilon will be 0.01\n",
    "LAMBDA = - math.log(0.01) / EXPLORATION_STOP  # speed of decay\n",
    "\n",
    "UPDATE_TARGET_FREQUENCY = 10000\n",
    "\n",
    "class Agent:\n",
    "    steps = 0\n",
    "    epsilon = MAX_EPSILON\n",
    "\n",
    "    def __init__(self, stateCnt, actionCnt):\n",
    "        self.stateCnt = stateCnt\n",
    "        self.actionCnt = actionCnt\n",
    "\n",
    "        self.brain = Brain(stateCnt, actionCnt)\n",
    "        # self.memory = Memory(MEMORY_CAPACITY)\n",
    "        \n",
    "    def act(self, s):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.actionCnt-1)\n",
    "        else:\n",
    "            return numpy.argmax(self.brain.predictOne(s))\n",
    "\n",
    "    def observe(self, sample):  # in (s, a, r, s_) format\n",
    "        x, y, errors = self._getTargets([(0, sample)])\n",
    "        self.memory.add(errors[0], sample)\n",
    "\n",
    "        if self.steps % UPDATE_TARGET_FREQUENCY == 0:\n",
    "            self.brain.updateTargetModel()\n",
    "\n",
    "        # slowly decrease Epsilon based on our eperience\n",
    "        self.steps += 1\n",
    "        self.epsilon = MIN_EPSILON + (MAX_EPSILON - MIN_EPSILON) * math.exp(-LAMBDA * self.steps)\n",
    "\n",
    "    def _getTargets(self, batch):\n",
    "        no_state = numpy.zeros(self.stateCnt)\n",
    "\n",
    "        states = numpy.array([ o[1][0] for o in batch ])\n",
    "        states_ = numpy.array([ (no_state if o[1][3] is None else o[1][3]) for o in batch ])\n",
    "\n",
    "        p = agent.brain.predict(states)\n",
    "\n",
    "        p_ = agent.brain.predict(states_, target=False)\n",
    "        pTarget_ = agent.brain.predict(states_, target=True)\n",
    "\n",
    "        x = numpy.zeros((len(batch), IMAGE_STACK, IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        y = numpy.zeros((len(batch), self.actionCnt))\n",
    "        errors = numpy.zeros(len(batch))\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            o = batch[i][1]\n",
    "            s = o[0]; a = o[1]; r = o[2]; s_ = o[3]\n",
    "            \n",
    "            t = p[i]\n",
    "            oldVal = t[a]\n",
    "            if s_ is None:\n",
    "                t[a] = r\n",
    "            else:\n",
    "                t[a] = r + GAMMA * pTarget_[i][ numpy.argmax(p_[i]) ]  # double DQN\n",
    "\n",
    "            x[i] = s\n",
    "            y[i] = t\n",
    "            errors[i] = abs(oldVal - t[a])\n",
    "\n",
    "        return (x, y, errors)\n",
    "\n",
    "    def replay(self):    \n",
    "        batch = self.memory.sample(BATCH_SIZE)\n",
    "        x, y, errors = self._getTargets(batch)\n",
    "\n",
    "        #update errors\n",
    "        for i in range(len(batch)):\n",
    "            idx = batch[i][0]\n",
    "            self.memory.update(idx, errors[i])\n",
    "\n",
    "        self.brain.train(x, y)\n",
    "\n",
    "class RandomAgent:\n",
    "    memory = Memory(MEMORY_CAPACITY)\n",
    "    exp = 0\n",
    "\n",
    "    def __init__(self, actionCnt):\n",
    "        self.actionCnt = actionCnt\n",
    "\n",
    "    def act(self, s):\n",
    "        return random.randint(0, self.actionCnt-1)\n",
    "\n",
    "    def observe(self, sample):  # in (s, a, r, s_) format\n",
    "        error = abs(sample[2])  # reward\n",
    "        self.memory.add(error, sample)\n",
    "        self.exp += 1\n",
    "\n",
    "    def replay(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, problem):\n",
    "        self.problem = problem\n",
    "        self.env = gym.make(problem)\n",
    "\n",
    "    def run(self, agent):                \n",
    "        img = self.env.reset()\n",
    "        w = processImage(img)\n",
    "        s = numpy.array([w, w])\n",
    "        \n",
    "        R = 0\n",
    "        \n",
    "        while True:         \n",
    "            # self.env.render()\n",
    "            a = agent.act(s)\n",
    "\n",
    "            r = 0\n",
    "            img, r, done, info = self.env.step(a)\n",
    "            s_ = numpy.array([s[1], processImage(img)]) #last two screens\n",
    "\n",
    "            r = np.clip(r, -1, 1)   # clip reward to [-1, 1]\n",
    "\n",
    "            if done: # terminal state\n",
    "                s_ = None\n",
    "\n",
    "            agent.observe( (s, a, r, s_) )\n",
    "            agent.replay()            \n",
    "\n",
    "            s = s_\n",
    "            R += r\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "#         print(\"Total reward:\", R)\n",
    "        return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-e9d16fd1fe7d>:8: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Initialization with random agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schandrasekhar/envs/kerasenvp3.6/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting learning\n",
      "WARNING:tensorflow:From /home/schandrasekhar/envs/kerasenvp3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schandrasekhar/envs/kerasenvp3.6/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "PROBLEM = 'Seaquest-v0'\n",
    "env = Environment(PROBLEM)\n",
    "\n",
    "stateCnt  = (IMAGE_STACK, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "actionCnt = env.env.action_space.n\n",
    "\n",
    "agent = Agent(stateCnt, actionCnt)\n",
    "randomAgent = RandomAgent(actionCnt)\n",
    "Rewards = []\n",
    "try:\n",
    "    print(\"Initialization with random agent...\")\n",
    "    while randomAgent.exp < MEMORY_CAPACITY:\n",
    "        env.run(randomAgent)\n",
    "        #print(randomAgent.exp, \"/\", MEMORY_CAPACITY)\n",
    "\n",
    "    agent.memory = randomAgent.memory\n",
    "\n",
    "    randomAgent = None\n",
    "\n",
    "    print(\"Starting learning\")\n",
    "    while True:\n",
    "        R = env.run(agent)\n",
    "        Rewards.append(R)\n",
    "finally:\n",
    "    print(\"done...\")\n",
    "#     agent.brain.model.save(\"Seaquest-DQN-PER.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
